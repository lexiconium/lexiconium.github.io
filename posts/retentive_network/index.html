<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Retentive Network: A Successor to Transformer for Large Language Models 리뷰 | What Did I Say?</title><meta name=keywords content="review"><meta name=description content="트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.
Fig. 1. &ldquo;Impossible triangle&rdquo;. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]
이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다."><meta name=author content="Minsoo Kim"><link rel=canonical href=https://lexiconium.github.io/posts/retentive_network/><link crossorigin=anonymous href=/assets/css/stylesheet.3aff192062f4c60f7b6df9f65a5ed72e7b9ac60ba7d03f69a8de17381072e402.css integrity="sha256-Ov8ZIGL0xg97bfn2Wl7XLnuaxgun0D9pqN4XOBBy5AI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://lexiconium.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lexiconium.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lexiconium.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://lexiconium.github.io/apple-touch-icon.png><link rel=mask-icon href=https://lexiconium.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-JVQGV39FHJ"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JVQGV39FHJ")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Retentive Network: A Successor to Transformer for Large Language Models 리뷰"><meta property="og:description" content="트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.
Fig. 1. &ldquo;Impossible triangle&rdquo;. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]
이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다."><meta property="og:type" content="article"><meta property="og:url" content="https://lexiconium.github.io/posts/retentive_network/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-19T17:10:12+09:00"><meta property="article:modified_time" content="2023-07-19T17:10:12+09:00"><meta property="og:site_name" content="What Did I Say?"><meta name=twitter:card content="summary"><meta name=twitter:title content="Retentive Network: A Successor to Transformer for Large Language Models 리뷰"><meta name=twitter:description content="트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.
Fig. 1. &ldquo;Impossible triangle&rdquo;. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]
이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://lexiconium.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Retentive Network: A Successor to Transformer for Large Language Models 리뷰","item":"https://lexiconium.github.io/posts/retentive_network/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Retentive Network: A Successor to Transformer for Large Language Models 리뷰","name":"Retentive Network: A Successor to Transformer for Large Language Models 리뷰","description":"트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.\nFig. 1. \u0026ldquo;Impossible triangle\u0026rdquo;. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]\n이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다.","keywords":["review"],"articleBody":"트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.\nFig. 1. “Impossible triangle”. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]\n이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다. 동시에 이 메커니즘에 대한 그들의 자신감을 Fig. 1과 함께 Arthur C. Clarke의 문구를 인용하여 표현하는데, 과연 가슴 설레는 문구가 아닐 수 없습니다.\n\"The only way to discover the limits of the possible is to go beyond them into the impossible.\" — Arthur C. Clark Retentive Network RetNet은 트랜스포머와 비슷하게 동일한 L개의 블럭이 쌓인 형태로 구성되어 있고, 각 블럭은 multi-scale retention (MSR) 모듈과 feed-forward network (FFN) 모듈로 이루어져 있습니다. RetNet은 주어진 인풋 시퀀스를 autoregressive하게 인코딩하는데, 그 과정은 다음과 같습니다. $$ \\begin{align} X^0 = \\left [ \\boldsymbol x_0, \\cdots , \\boldsymbol x_{\\lvert x \\rvert} \\right ] \\in \\mathbb{R}^{\\lvert x \\rvert \\times d_\\text{model}} \\end{align} $$\n$$ \\begin{align} X^l = \\text{RetNet}_l \\left ( X^{l-1} \\right ),\\ l \\in \\left [ 1, L \\right ] \\end{align} $$\n이 또한 트랜스포머와 유사한데, 인풋 시퀀스 \\(x = x_1 \\cdots x_{\\lvert x \\rvert}\\)를 (1) 시퀀스를 임베딩 벡터로 변환해주고, (2) 이전 블럭의 아웃풋을 이후 블럭의 인풋으로 사용하는 것의 반복입니다.\n들어가기에 앞서\n아래 나오는 \\( V_n, O_n \\) 등은 실제론 벡터로, 논문에서도 설명의 간편함을 위해 스칼라 사이의 mapping으로 설명하고 있습니다. 또한 논문에선 이를 구분하기 위해 소문자 표현을 사용했지만 저는 다음 규칙에 맞춰 작성하겠습니다.\nMatrix Vector Scalar \\( \\boldsymbol{A} \\) \\( \\vec {\\boldsymbol{A}} \\) \\( A \\) Retention Fig. 2. RetNet의 dual form.\nRetention mechanism의 가장 큰 특징은 두 가지 표현 방식을 가지는 것으로, 학습시엔 parallelism의 이점을, 추론시엔 RNN으로서의 이점을 활용합니다. 이를 위해 연구진은 어떤 recurrent state \\( \\vec {\\boldsymbol S}_n \\)을 사용하여 \\( V_n \\mapsto O_n \\)의 sequence modelling problem을 고려합니다. 당장 \\( V_n, O_n \\)등이 뜻하는 바가 뭔지 궁금하신 분들도 계시겠지만 이는 잠깐동안 미뤄놓겠습니다. 연구진은 recurrent state \\( \\vec {\\boldsymbol S}_n \\)과 \\( O_n \\)으로의 mapping을 다음과 같이 정의했습니다.\n$$ \\begin{align} \\vec {\\boldsymbol S_n} = \\boldsymbol{A} \\vec {\\boldsymbol S}_{n-1} + \\vec {\\boldsymbol K}_n V_n \\end{align} $$\n$$ \\begin{align} O_n = \\vec {\\boldsymbol Q}_n \\cdot \\vec {\\boldsymbol S}_n \\end{align} $$\n이때 \\( \\vec {\\boldsymbol Q}_n, \\vec {\\boldsymbol K}_n, V_n \\)는 아래와 같습니다. \\( V_n \\)이 스칼라라는 것만 제외하면 익숙한 형태입니다. 참고로 “들어가기에 앞서\"에 설명했듯 이는 설명의 간편함을 위한 것으로, 실제 모델에선 \\( \\vec {\\boldsymbol V}_n \\) 또한 벡터고 \\({\\boldsymbol W}_V\\)도 매트릭스 입니다. $$ \\begin{align} \\vec {\\boldsymbol Q}_n \u0026= \\vec {\\boldsymbol X}_n {\\boldsymbol W}_Q \\\\ \\vec {\\boldsymbol K}_n \u0026= \\vec {\\boldsymbol X}_n {\\boldsymbol W}_K \\\\ V_n \u0026= \\vec {\\boldsymbol X}_n \\cdot \\vec {\\boldsymbol W}_V \\end{align} $$\n다시 위로 돌아가서 식 (4)에 식 (3)을 대입해 \\( O_n \\)을 \\( \\vec {\\boldsymbol S}_i \\) 없이 표현할 수 있는지 보겠습니다.\n$$ \\begin{align} O_n \u0026= \\vec {\\boldsymbol Q}_n \\cdot \\vec {\\boldsymbol S}_n \\\\ \u0026= \\vec {\\boldsymbol Q}_n \\cdot \\left ( \\boldsymbol{A} \\vec {\\boldsymbol S}_{n-1} + \\vec {\\boldsymbol K}_n V_n \\right ) \\\\ \u0026= \\vec {\\boldsymbol Q}_n \\cdot \\left ( \\boldsymbol{A} \\left ( \\boldsymbol{A} \\vec {\\boldsymbol S}_{n-2} + \\vec {\\boldsymbol K}_{n-1} V_{n-1} \\right ) + \\vec {\\boldsymbol K}_n V_n \\right ) \\\\ \u0026= \\vec {\\boldsymbol Q}_n \\cdot \\left ( {\\boldsymbol A}^n \\vec {\\boldsymbol S}_0 + \\sum^n_{m=1} {\\boldsymbol A}^{n-m} \\vec {\\boldsymbol K}_m V_m \\right ) \\end{align} $$\n만약 \\( \\vec {\\boldsymbol S}_0 = \\vec 0 \\)이라면 식 (11)에서 state항을 없앨 수 있고, 논문에서 이를 명시적으로 서술하진 않지만 식 (12)에 해당하는 수식만 적혀있는걸 보면 \\( \\vec {\\boldsymbol S}_0 \\)는 \\( \\vec 0 \\)으로 초기화된다고 추측할 수 있습니다.\n$$ \\begin{align} O_n = \\vec {\\boldsymbol Q}_n \\cdot \\sum^n_{m=1} {\\boldsymbol A}^{n-m} \\vec {\\boldsymbol K}_m V_m \\end{align} $$\n여기서 더 나아가 \\( \\boldsymbol{A} \\)를 대각화 해주면 \\( \\boldsymbol{A} = \\boldsymbol{\\Lambda}\\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right ) \\boldsymbol{\\Lambda}^{-1} \\)로 표현할 수 있고, \\( \\boldsymbol{A}^{n-m} \\)은 다음과 같습니다.\n$$ \\begin{align} \\boldsymbol{A}^{n-m} = \\boldsymbol{\\Lambda}\\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{n-m} \\boldsymbol{\\Lambda}^{-1} \\end{align} $$\n이제 식 (13)을 식 (12)에 대입하여 정리해보겠습니다.\n$$ \\begin{align} O_n \u0026= \\sum^n_{m=1} \\vec {\\boldsymbol Q}_n \\cdot \\boldsymbol{\\Lambda}\\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{n-m} \\boldsymbol{\\Lambda}^{-1} \\vec {\\boldsymbol K}_m V_m \\\\ \u0026= \\sum^n_{m=1} \\vec {\\boldsymbol Q}^{\\prime}_n \\cdot \\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{n-m} \\vec {\\boldsymbol K}^{\\prime}_m V_m \\\\ \u0026= \\sum^n_{m=1} \\left ( \\vec {\\boldsymbol Q}^{\\prime}_n \\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^n \\right ) \\cdot \\left ( \\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{-m} \\vec {\\boldsymbol K}^{\\prime}_m \\right ) V_m \\end{align} $$\n\\( {\\boldsymbol W}_Q, {\\boldsymbol W}_K \\)가 학습 가능한 파라미터라는 점과 \\( \\boldsymbol{\\Lambda} \\)가 인풋으로부터 독립적인 값이라는 점을 고려해 프라임 표시를 떼고, \\( \\vec {\\boldsymbol \\gamma} \\)를 scalar로 reduce하면 \\( O_n \\)은 다음과 같아집니다.\n$$ \\begin{align} O_n = \\sum^n_{m=1} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\right ) \\cdot \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m \\end{align} $$\n\\( \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}}, \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\)는 xPos [2]의 형태로, 생긴대로 시퀀스 상에서의 위치 정보를 포함하고 있으며, relative position embedding의 일종으로 해석될 수 있습니다. 또한 식 (17)은 쉽게 parallelizable한 형태를 띄고 있으며, 이는 식 (4)의 recurrent한 형태와 함께 \\( O_n\\)이 두 가지 방식으로 계산될 수 있다는 것을 보여줍니다.\n이제 \\( O_n\\)을 attention score와 유사한 역할을 하는 어떤 값이라고 생각해보면, retention mechanism은 이 값을 학습시엔 트랜스포머처럼 병렬적으로 계산하지만 추론시엔 같은 파라미터를 이용함에도 recurrent하게 계산할 수 있어 quadratic한 코스트를 linear하게 줄일 수 있도록 하는 메커니즘이라고 할 수 있습니다.\nChunkwise Recurrent Representation 앞서 recurrent, parallel representation을 살펴봤는데 연구진은 여기서 그치지 않고 그 둘의 표현법을 섞어 활용하기도 했습니다. 우선 \\( \\vec{\\boldsymbol R}_n = \\sum^n_{m=1} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m \\)이라고 정의하면 식 (17)을 다음과 \\(b\\)의 길이만큼 잘라서 표현할 수 있습니다.\n$$ \\vec{\\boldsymbol R}_n = \\sum^n_{m=b} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m + \\gamma^b \\vec{\\boldsymbol R}_{n-b} $$\n즉, 각각의 chunk는 그대로 식 (17) 혹은 식 (19)의 inner-chunk와 같이 계산하고, chunk들 사이의 정보는 아래 식 (19)의 cross-chunk와 같이 계산할 수 있습니다. 이를 통해 연구진은 매우 긴 시퀀스도 일정한 길이의 chunk로 잘라 계산함으로써 학습 효율을 높일 수 있었다고 합니다.\n$$ \\begin{align} O_n \u0026= \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\cdot \\vec{\\boldsymbol R}_n \\\\ \u0026= \\underbrace{\\sum^n_{m=b} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\right ) \\cdot \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m }_\\text{Inner-Chunk} + \\overbrace{\\gamma^b \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\cdot \\vec{\\boldsymbol R}_{n-b} }^\\text{Cross-Chunk} \\end{align} $$\nGated Multi-Scale Retention Multi-Scale Retention (MSR) 또한 Multi-Head Attention (MHA)과 유사하게 \\(d_\\text{model}\\) 차원을 헤드의 차원 \\( d \\)로 나눠 여러 헤드를 사용했습니다. 각 헤드는 서로 다른 \\( {\\boldsymbol W}_Q, {\\boldsymbol W}_K, {\\boldsymbol W}_V \\in \\mathbb{R}^{d \\times d} \\) 파라미터를 가지며 각 헤드의 \\( \\gamma \\)는 다르지만, 레이어 별로는 동일합니다. 또한 비선형성을 증가시키기 위해 swish gate를 추가하였다고 합니다.\n$$ \\begin{align} \\gamma \u0026= 1 - 2^{-5-\\text{arange}(0, h)} \\in \\mathbb R^h \\\\ \\text{head}_i \u0026= \\text{Retention}(\\boldsymbol X, \\gamma_i ) \\\\ Y \u0026= \\text{GroupNorm}_h (\\text{Concat}(\\text{head}_1, \\cdots, \\text{head}_h)) \\\\ MSR(\\boldsymbol X) \u0026= (\\text{swish}({\\boldsymbol X} {\\boldsymbol W}_G) \\odot Y) {\\boldsymbol W}_O \\end{align} $$\n이때 \\( {\\boldsymbol W}_G, {\\boldsymbol W}_O \\in \\mathbb{R}^{d_\\text{model} \\times d_\\text{model}} \\) 역시 학습가능한 파라미터입니다.\n이외에도 Normalization 기법이나 pseudocode 등의 내용 또한 있으니 이런 보다 디테일한 사항이 궁금하신 분은 논문을 참고하시길 바랍니다.\nOverall Architecture of Retention Networks $$ \\begin{align} {\\boldsymbol Y}^l \u0026= \\text{MSR}(\\text{LN}({\\boldsymbol X}^l)) + {\\boldsymbol X}^l \\\\ {\\boldsymbol X}^{l+1} \u0026= \\text{FFN}(\\text{LN}({\\boldsymbol Y}^l)) + {\\boldsymbol Y}^l \\end{align} $$\n이때 \\( \\text{FFN}({\\boldsymbol X}) = \\text{gelu}({\\boldsymbol X}{\\boldsymbol W}_1) {\\boldsymbol W}_2 \\)의 일반적인 형태입니다.\nExperiments 대망의 성능을 볼 차례가 되었습니다. 디테일한 사항은 논문에 남겨두고, 간략하게 살펴보도록 하겠습니다.\nTable. 1. 크기와 학습 하이퍼 파라미터\nFig. 3. 모델 사이즈에 따른 perplexity 추이.\nTable. 2. 6.7B 모델의 Zero-Shot, 4-Shot 성능\nTable. 3. 크기별 학습 코스트. 추론 뿐만 아니라 학습시에도 트랜스포머 대비 효율적임을 볼 수 있습니다.\nFig. 4. 추론 코스트. Recurrent한 형태로 추론하기에 시퀀스 길이에 무관한 코스트를 보여줍니다.\nTable. 4. Ablation.\n마무리 무려 트랜스포머의 후계자라는 제목을 달고나온 논문으로, 몇 안되는 실험 결과로 확언할 순 없지만, 그 성능 격차가 트랜스포머의 첫 등장만큼 놀랍지는 않다고 생각합니다. 동시에 트랜스포머와 비슷하거나 조금 높은 성능을 보이는 동시에 학습과 추론시 코스트는 드라마틱하게 줄였으니 정말 논문의 내용처럼 뛰어난 점만 있다면, 시장이나 사회적 측면에서의 파급력은 상당할 수도 있지 않을까 하는 생각도 듭니다.\n다만 추론시 RNN처럼 state에만 의존하는 것은 아님에도, 어쨌든 retention score를 계산함에 있어 state에 인코딩된 context 정보에 의존한다는 점이 아주 긴 시퀀스에서의 성능을 궁금하게 만듭니다. 또한 autoregressive하게 생성된 문장의 퀄리티 또한 알고 싶지만 이런 예시가 제공되지 않아 다소 아쉽다고 느껴집니다.\n그럼에도 불구하고 dual form을 취한다는 발상과 그 결과물 자체는 놀랍습니다. 어떻게 보면 보다 transformer라는 명칭이 어울리는건 이쪽일지도 모르겠다는 생각을 하며 이만 마무리 하겠습니다.\n여담 현재 2023년 7월 20일 기준으로 다음주에 코드를 공개한다고 하니, 관심 있으신 분은 추후 링크를 참고하시길 바랍니다.\nReferences [1] Sun et al. “Retentive Network: A Successor to Transformer for Large Language Models” (2023).\n[2] Sun et al. “A Length-Extrapolatable Transformer” (2022).\n","wordCount":"1476","inLanguage":"en","datePublished":"2023-07-19T17:10:12+09:00","dateModified":"2023-07-19T17:10:12+09:00","author":{"@type":"Person","name":"Minsoo Kim"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://lexiconium.github.io/posts/retentive_network/"},"publisher":{"@type":"Organization","name":"What Did I Say?","logo":{"@type":"ImageObject","url":"https://lexiconium.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lexiconium.github.io accesskey=h title="What Did I Say? (Alt + H)">What Did I Say?</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lexiconium.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://lexiconium.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://lexiconium.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Retentive Network: A Successor to Transformer for Large Language Models 리뷰</h1><div class=post-meta><span title='2023-07-19 17:10:12 +0900 +0900'>July 19, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Minsoo Kim</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#retentive-network aria-label="Retentive Network">Retentive Network</a><ul><li><a href=#retention aria-label=Retention>Retention</a><ul><li><a href=#chunkwise-recurrent-representation aria-label="Chunkwise Recurrent Representation">Chunkwise Recurrent Representation</a></li></ul></li><li><a href=#gated-multi-scale-retention aria-label="Gated Multi-Scale Retention">Gated Multi-Scale Retention</a></li><li><a href=#overall-architecture-of-retention-networks aria-label="Overall Architecture of Retention Networks">Overall Architecture of Retention Networks</a></li></ul></li><li><a href=#experiments aria-label=Experiments>Experiments</a></li><li><a href=#%eb%a7%88%eb%ac%b4%eb%a6%ac aria-label=마무리>마무리</a></li><li><a href=#%ec%97%ac%eb%8b%b4 aria-label=여담>여담</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.</p><p><img loading=lazy src=./images/impossible_triangle.png alt=impossible_triangle></p><p>Fig. 1. &ldquo;Impossible triangle&rdquo;. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]</p><p>이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다. 동시에 이 메커니즘에 대한 그들의 자신감을 Fig. 1과 함께 Arthur C. Clarke의 문구를 인용하여 표현하는데, 과연 가슴 설레는 문구가 아닐 수 없습니다.</p><center><blockquote>"The only way to discover the limits of the possible is to go beyond them into the impossible."<figcaption>&mdash; <cite>Arthur C. Clark</cite></figcaption></blockquote></center><h2 id=retentive-network>Retentive Network<a hidden class=anchor aria-hidden=true href=#retentive-network>#</a></h2><p>RetNet은 트랜스포머와 비슷하게 동일한 <em>L</em>개의 블럭이 쌓인 형태로 구성되어 있고, 각 블럭은 multi-scale retention (MSR) 모듈과 feed-forward network (FFN) 모듈로 이루어져 있습니다. RetNet은 주어진 인풋 시퀀스를 autoregressive하게 인코딩하는데, 그 과정은 다음과 같습니다.
$$
\begin{align}
X^0 = \left [ \boldsymbol x_0, \cdots , \boldsymbol x_{\lvert x \rvert} \right ] \in \mathbb{R}^{\lvert x \rvert \times d_\text{model}}
\end{align}
$$</p><p>$$
\begin{align}
X^l = \text{RetNet}_l \left ( X^{l-1} \right ),\ l \in \left [ 1, L \right ]
\end{align}
$$</p><p>이 또한 트랜스포머와 유사한데, 인풋 시퀀스 \(x = x_1 \cdots x_{\lvert x \rvert}\)를 (1) 시퀀스를 임베딩 벡터로 변환해주고, (2) 이전 블럭의 아웃풋을 이후 블럭의 인풋으로 사용하는 것의 반복입니다.</p><blockquote><p><strong>들어가기에 앞서</strong></p><p>아래 나오는 \( V_n, O_n \) 등은 실제론 벡터로, 논문에서도 설명의 간편함을 위해 스칼라 사이의 mapping으로 설명하고 있습니다. 또한 논문에선 이를 구분하기 위해 소문자 표현을 사용했지만 저는 다음 규칙에 맞춰 작성하겠습니다.</p><table><thead><tr><th style=text-align:center>Matrix</th><th style=text-align:center>Vector</th><th style=text-align:center>Scalar</th></tr></thead><tbody><tr><td style=text-align:center>\( \boldsymbol{A} \)</td><td style=text-align:center>\( \vec {\boldsymbol{A}} \)</td><td style=text-align:center>\( A \)</td></tr></tbody></table></blockquote><h3 id=retention>Retention<a hidden class=anchor aria-hidden=true href=#retention>#</a></h3><p><img loading=lazy src=./images/retnet.png alt=retnet></p><p>Fig. 2. RetNet의 dual form.</p><p>Retention mechanism의 가장 큰 특징은 두 가지 표현 방식을 가지는 것으로, 학습시엔 parallelism의 이점을, 추론시엔 RNN으로서의 이점을 활용합니다. 이를 위해 연구진은 어떤 recurrent state \( \vec {\boldsymbol S}_n \)을 사용하여 \( V_n \mapsto O_n \)의 sequence modelling problem을 고려합니다. 당장 \( V_n, O_n \)등이 뜻하는 바가 뭔지 궁금하신 분들도 계시겠지만 이는 잠깐동안 미뤄놓겠습니다. 연구진은 recurrent state \( \vec {\boldsymbol S}_n \)과 \( O_n \)으로의 mapping을 다음과 같이 정의했습니다.</p><p>$$
\begin{align}
\vec {\boldsymbol S_n} = \boldsymbol{A} \vec {\boldsymbol S}_{n-1} + \vec {\boldsymbol K}_n V_n
\end{align}
$$</p><p>$$
\begin{align}
O_n = \vec {\boldsymbol Q}_n \cdot \vec {\boldsymbol S}_n
\end{align}
$$</p><p>이때 \( \vec {\boldsymbol Q}_n, \vec {\boldsymbol K}_n, V_n \)는 아래와 같습니다. \( V_n \)이 스칼라라는 것만 제외하면 익숙한 형태입니다. 참고로 &ldquo;들어가기에 앞서"에 설명했듯 이는 설명의 간편함을 위한 것으로, 실제 모델에선 \( \vec {\boldsymbol V}_n \) 또한 벡터고 \({\boldsymbol W}_V\)도 매트릭스 입니다.
$$
\begin{align}
\vec {\boldsymbol Q}_n &= \vec {\boldsymbol X}_n {\boldsymbol W}_Q \\
\vec {\boldsymbol K}_n &= \vec {\boldsymbol X}_n {\boldsymbol W}_K \\
V_n &= \vec {\boldsymbol X}_n \cdot \vec {\boldsymbol W}_V
\end{align}
$$</p><p>다시 위로 돌아가서 식 (4)에 식 (3)을 대입해 \( O_n \)을 \( \vec {\boldsymbol S}_i \) 없이 표현할 수 있는지 보겠습니다.</p><p>$$
\begin{align}
O_n &= \vec {\boldsymbol Q}_n \cdot \vec {\boldsymbol S}_n \\
&= \vec {\boldsymbol Q}_n \cdot \left ( \boldsymbol{A} \vec {\boldsymbol S}_{n-1} + \vec {\boldsymbol K}_n V_n \right ) \\
&= \vec {\boldsymbol Q}_n \cdot \left (
\boldsymbol{A} \left (
\boldsymbol{A} \vec {\boldsymbol S}_{n-2} + \vec {\boldsymbol K}_{n-1} V_{n-1}
\right ) + \vec {\boldsymbol K}_n V_n
\right ) \\
&= \vec {\boldsymbol Q}_n \cdot
\left (
{\boldsymbol A}^n \vec {\boldsymbol S}_0 +
\sum^n_{m=1} {\boldsymbol A}^{n-m} \vec {\boldsymbol K}_m V_m
\right )
\end{align}
$$</p><p>만약 \( \vec {\boldsymbol S}_0 = \vec 0 \)이라면 식 (11)에서 state항을 없앨 수 있고, 논문에서 이를 명시적으로 서술하진 않지만 식 (12)에 해당하는 수식만 적혀있는걸 보면 \( \vec {\boldsymbol S}_0 \)는 \( \vec 0 \)으로 초기화된다고 추측할 수 있습니다.</p><p>$$
\begin{align}
O_n = \vec {\boldsymbol Q}_n \cdot \sum^n_{m=1} {\boldsymbol A}^{n-m} \vec {\boldsymbol K}_m V_m
\end{align}
$$</p><p>여기서 더 나아가 \( \boldsymbol{A} \)를 대각화 해주면 \( \boldsymbol{A} = \boldsymbol{\Lambda}\left ( \vec {\boldsymbol{\gamma}} e^{i \vec {\boldsymbol \theta}} \right ) \boldsymbol{\Lambda}^{-1} \)로 표현할 수 있고, \( \boldsymbol{A}^{n-m} \)은 다음과 같습니다.</p><p>$$
\begin{align}
\boldsymbol{A}^{n-m} = \boldsymbol{\Lambda}\left (
\vec {\boldsymbol{\gamma}} e^{i \vec {\boldsymbol \theta}}
\right )^{n-m} \boldsymbol{\Lambda}^{-1}
\end{align}
$$</p><p>이제 식 (13)을 식 (12)에 대입하여 정리해보겠습니다.</p><p>$$
\begin{align}
O_n &= \sum^n_{m=1}
\vec {\boldsymbol Q}_n \cdot \boldsymbol{\Lambda}\left (
\vec {\boldsymbol{\gamma}} e^{i \vec {\boldsymbol \theta}}
\right )^{n-m} \boldsymbol{\Lambda}^{-1} \vec {\boldsymbol K}_m V_m \\
&= \sum^n_{m=1}
\vec {\boldsymbol Q}^{\prime}_n \cdot \left (
\vec {\boldsymbol{\gamma}} e^{i \vec {\boldsymbol \theta}}
\right )^{n-m} \vec {\boldsymbol K}^{\prime}_m V_m \\
&= \sum^n_{m=1}
\left (
\vec {\boldsymbol Q}^{\prime}_n
\left ( \vec {\boldsymbol{\gamma}} e^{i \vec {\boldsymbol \theta}} \right )^n
\right )
\cdot
\left (
\left ( \vec {\boldsymbol{\gamma}} e^{i \vec {\boldsymbol \theta}} \right )^{-m}
\vec {\boldsymbol K}^{\prime}_m
\right )
V_m
\end{align}
$$</p><p>\( {\boldsymbol W}_Q, {\boldsymbol W}_K \)가 학습 가능한 파라미터라는 점과 \( \boldsymbol{\Lambda} \)가 인풋으로부터 독립적인 값이라는 점을 고려해 프라임 표시를 떼고, \( \vec {\boldsymbol \gamma} \)를 scalar로 reduce하면 \( O_n \)은 다음과 같아집니다.</p><p>$$
\begin{align}
O_n = \sum^n_{m=1}
\gamma^{n-m}
\left (
\vec {\boldsymbol Q}_n e^{i n \vec {\boldsymbol \theta}}
\right )
\cdot
\left (
\vec {\boldsymbol K}_m e^{i m \vec {\boldsymbol \theta}}
\right )^\dagger
V_m
\end{align}
$$</p><p>\( \vec {\boldsymbol Q}_n e^{i n \vec {\boldsymbol \theta}}, \vec {\boldsymbol K}_m e^{i m \vec {\boldsymbol \theta}} \)는 xPos [2]의 형태로, 생긴대로 시퀀스 상에서의 위치 정보를 포함하고 있으며, relative position embedding의 일종으로 해석될 수 있습니다. 또한 식 (17)은 쉽게 parallelizable한 형태를 띄고 있으며, 이는 식 (4)의 recurrent한 형태와 함께 \( O_n\)이 두 가지 방식으로 계산될 수 있다는 것을 보여줍니다.</p><p>이제 \( O_n\)을 attention score와 유사한 역할을 하는 어떤 값이라고 생각해보면, retention mechanism은 이 값을 학습시엔 트랜스포머처럼 병렬적으로 계산하지만 추론시엔 같은 파라미터를 이용함에도 recurrent하게 계산할 수 있어 quadratic한 코스트를 linear하게 줄일 수 있도록 하는 메커니즘이라고 할 수 있습니다.</p><h4 id=chunkwise-recurrent-representation>Chunkwise Recurrent Representation<a hidden class=anchor aria-hidden=true href=#chunkwise-recurrent-representation>#</a></h4><p>앞서 recurrent, parallel representation을 살펴봤는데 연구진은 여기서 그치지 않고 그 둘의 표현법을 섞어 활용하기도 했습니다. 우선 \( \vec{\boldsymbol R}_n = \sum^n_{m=1} \gamma^{n-m} \left ( \vec {\boldsymbol K}_m e^{i m \vec {\boldsymbol \theta}} \right )^\dagger V_m \)이라고 정의하면 식 (17)을 다음과 \(b\)의 길이만큼 잘라서 표현할 수 있습니다.</p><p>$$
\vec{\boldsymbol R}_n =
\sum^n_{m=b} \gamma^{n-m} \left (
\vec {\boldsymbol K}_m e^{i m \vec {\boldsymbol \theta}}
\right )^\dagger V_m
+
\gamma^b \vec{\boldsymbol R}_{n-b}
$$</p><p>즉, 각각의 chunk는 그대로 식 (17) 혹은 식 (19)의 inner-chunk와 같이 계산하고, chunk들 사이의 정보는 아래 식 (19)의 cross-chunk와 같이 계산할 수 있습니다. 이를 통해 연구진은 매우 긴 시퀀스도 일정한 길이의 chunk로 잘라 계산함으로써 학습 효율을 높일 수 있었다고 합니다.</p><p>$$
\begin{align}
O_n &= \vec {\boldsymbol Q}_n e^{i n \vec {\boldsymbol \theta}} \cdot \vec{\boldsymbol R}_n \\
&= \underbrace{\sum^n_{m=b} \gamma^{n-m}
\left ( \vec {\boldsymbol Q}_n e^{i n \vec {\boldsymbol \theta}} \right )
\cdot
\left ( \vec {\boldsymbol K}_m e^{i m \vec {\boldsymbol \theta}} \right )^\dagger V_m
}_\text{Inner-Chunk}
+
\overbrace{\gamma^b \vec {\boldsymbol Q}_n e^{i n \vec {\boldsymbol \theta}} \cdot \vec{\boldsymbol R}_{n-b}
}^\text{Cross-Chunk}
\end{align}
$$</p><h3 id=gated-multi-scale-retention>Gated Multi-Scale Retention<a hidden class=anchor aria-hidden=true href=#gated-multi-scale-retention>#</a></h3><p>Multi-Scale Retention (MSR) 또한 Multi-Head Attention (MHA)과 유사하게 \(d_\text{model}\) 차원을 헤드의 차원 \( d \)로 나눠 여러 헤드를 사용했습니다. 각 헤드는 서로 다른 \( {\boldsymbol W}_Q, {\boldsymbol W}_K, {\boldsymbol W}_V \in \mathbb{R}^{d \times d} \) 파라미터를 가지며 각 헤드의 \( \gamma \)는 다르지만, 레이어 별로는 동일합니다. 또한 비선형성을 증가시키기 위해 swish gate를 추가하였다고 합니다.</p><p>$$
\begin{align}
\gamma &= 1 - 2^{-5-\text{arange}(0, h)} \in \mathbb R^h \\
\text{head}_i &= \text{Retention}(\boldsymbol X, \gamma_i ) \\
Y &= \text{GroupNorm}_h (\text{Concat}(\text{head}_1, \cdots, \text{head}_h)) \\
MSR(\boldsymbol X) &= (\text{swish}({\boldsymbol X} {\boldsymbol W}_G) \odot Y) {\boldsymbol W}_O
\end{align}
$$</p><p>이때 \( {\boldsymbol W}_G, {\boldsymbol W}_O \in \mathbb{R}^{d_\text{model} \times d_\text{model}} \) 역시 학습가능한 파라미터입니다.</p><p>이외에도 Normalization 기법이나 pseudocode 등의 내용 또한 있으니 이런 보다 디테일한 사항이 궁금하신 분은 논문을 참고하시길 바랍니다.</p><h3 id=overall-architecture-of-retention-networks>Overall Architecture of Retention Networks<a hidden class=anchor aria-hidden=true href=#overall-architecture-of-retention-networks>#</a></h3><p>$$
\begin{align}
{\boldsymbol Y}^l &= \text{MSR}(\text{LN}({\boldsymbol X}^l)) + {\boldsymbol X}^l \\
{\boldsymbol X}^{l+1} &= \text{FFN}(\text{LN}({\boldsymbol Y}^l)) + {\boldsymbol Y}^l
\end{align}
$$</p><p>이때 \( \text{FFN}({\boldsymbol X}) = \text{gelu}({\boldsymbol X}{\boldsymbol W}_1) {\boldsymbol W}_2 \)의 일반적인 형태입니다.</p><h2 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h2><p>대망의 성능을 볼 차례가 되었습니다. 디테일한 사항은 논문에 남겨두고, 간략하게 살펴보도록 하겠습니다.</p><p><img loading=lazy src=./images/size_and_hp.png alt=size_and_hp></p><p>Table. 1. 크기와 학습 하이퍼 파라미터</p><p><img loading=lazy src=./images/perplexity.png alt=perplexity></p><p>Fig. 3. 모델 사이즈에 따른 perplexity 추이.</p><p><img loading=lazy src=./images/zero_few_shots.png alt=zero_few_shots></p><p>Table. 2. 6.7B 모델의 <em>Zero-Shot, 4-Shot</em> 성능</p><p><img loading=lazy src=./images/training_cost.png alt=training_cost></p><p>Table. 3. 크기별 학습 코스트. 추론 뿐만 아니라 학습시에도 트랜스포머 대비 효율적임을 볼 수 있습니다.</p><p><img loading=lazy src=./images/inference_cost.png alt=inference_cost></p><p>Fig. 4. 추론 코스트. Recurrent한 형태로 추론하기에 시퀀스 길이에 무관한 코스트를 보여줍니다.</p><p><img loading=lazy src=./images/ablation.png alt=ablation></p><p>Table. 4. Ablation.</p><h2 id=마무리>마무리<a hidden class=anchor aria-hidden=true href=#마무리>#</a></h2><p>무려 트랜스포머의 후계자라는 제목을 달고나온 논문으로, 몇 안되는 실험 결과로 확언할 순 없지만, 그 성능 격차가 트랜스포머의 첫 등장만큼 놀랍지는 않다고 생각합니다. 동시에 트랜스포머와 비슷하거나 조금 높은 성능을 보이는 동시에 학습과 추론시 코스트는 드라마틱하게 줄였으니 정말 논문의 내용처럼 뛰어난 점만 있다면, 시장이나 사회적 측면에서의 파급력은 상당할 수도 있지 않을까 하는 생각도 듭니다.</p><p>다만 추론시 RNN처럼 state에만 의존하는 것은 아님에도, 어쨌든 retention score를 계산함에 있어 state에 인코딩된 context 정보에 의존한다는 점이 아주 긴 시퀀스에서의 성능을 궁금하게 만듭니다. 또한 autoregressive하게 생성된 문장의 퀄리티 또한 알고 싶지만 이런 예시가 제공되지 않아 다소 아쉽다고 느껴집니다.</p><p>그럼에도 불구하고 dual form을 취한다는 발상과 그 결과물 자체는 놀랍습니다. 어떻게 보면 보다 transformer라는 명칭이 어울리는건 이쪽일지도 모르겠다는 생각을 하며 이만 마무리 하겠습니다.</p><h2 id=여담>여담<a hidden class=anchor aria-hidden=true href=#여담>#</a></h2><p>현재 2023년 7월 20일 기준으로 다음주에 코드를 공개한다고 하니, 관심 있으신 분은 추후 <a href=https://aka.ms/retnet>링크</a>를 참고하시길 바랍니다.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Sun et al. &ldquo;<a href=https://arxiv.org/abs/2307.08621>Retentive Network: A Successor to Transformer for Large Language Models</a>&rdquo; (2023).</p><p>[2] Sun et al. &ldquo;<a href=https://arxiv.org/abs/2212.10554>A Length-Extrapolatable Transformer</a>&rdquo; (2022).</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://lexiconium.github.io/tags/review/>review</a></li></ul><nav class=paginav><a class=next href=https://lexiconium.github.io/posts/openai_vpt/><span class=title>Next »</span><br><span>OpenAI의 Video PreTraining (VPT) 리뷰</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Retentive Network: A Successor to Transformer for Large Language Models 리뷰 on twitter" href="https://twitter.com/intent/tweet/?text=Retentive%20Network%3a%20A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models%20%eb%a6%ac%eb%b7%b0&amp;url=https%3a%2f%2flexiconium.github.io%2fposts%2fretentive_network%2f&amp;hashtags=review"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Retentive Network: A Successor to Transformer for Large Language Models 리뷰 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flexiconium.github.io%2fposts%2fretentive_network%2f&amp;title=Retentive%20Network%3a%20A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models%20%eb%a6%ac%eb%b7%b0&amp;summary=Retentive%20Network%3a%20A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models%20%eb%a6%ac%eb%b7%b0&amp;source=https%3a%2f%2flexiconium.github.io%2fposts%2fretentive_network%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Retentive Network: A Successor to Transformer for Large Language Models 리뷰 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flexiconium.github.io%2fposts%2fretentive_network%2f&title=Retentive%20Network%3a%20A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models%20%eb%a6%ac%eb%b7%b0"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Retentive Network: A Successor to Transformer for Large Language Models 리뷰 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flexiconium.github.io%2fposts%2fretentive_network%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Retentive Network: A Successor to Transformer for Large Language Models 리뷰 on whatsapp" href="https://api.whatsapp.com/send?text=Retentive%20Network%3a%20A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models%20%eb%a6%ac%eb%b7%b0%20-%20https%3a%2f%2flexiconium.github.io%2fposts%2fretentive_network%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Retentive Network: A Successor to Transformer for Large Language Models 리뷰 on telegram" href="https://telegram.me/share/url?text=Retentive%20Network%3a%20A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models%20%eb%a6%ac%eb%b7%b0&amp;url=https%3a%2f%2flexiconium.github.io%2fposts%2fretentive_network%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://lexiconium.github.io>What Did I Say?</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>