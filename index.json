[{"content":"트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.\nFig. 1. \u0026ldquo;Impossible triangle\u0026rdquo;. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]\n이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다. 동시에 이 메커니즘에 대한 그들의 자신감을 Fig. 1과 함께 Arthur C. Clarke의 문구를 인용하여 표현하는데, 과연 가슴 설레는 문구가 아닐 수 없습니다.\n\"The only way to discover the limits of the possible is to go beyond them into the impossible.\" \u0026mdash; Arthur C. Clark Retentive Network RetNet은 트랜스포머와 비슷하게 동일한 L개의 블럭이 쌓인 형태로 구성되어 있고, 각 블럭은 multi-scale retention (MSR) 모듈과 feed-forward network (FFN) 모듈로 이루어져 있습니다. RetNet은 주어진 인풋 시퀀스를 autoregressive하게 인코딩하는데, 그 과정은 다음과 같습니다. $$ \\begin{align} X^0 = \\left [ \\boldsymbol x_0, \\cdots , \\boldsymbol x_{\\lvert x \\rvert} \\right ] \\in \\mathbb{R}^{\\lvert x \\rvert \\times d_\\text{model}} \\end{align} $$\n$$ \\begin{align} X^l = \\text{RetNet}_l \\left ( X^{l-1} \\right ),\\ l \\in \\left [ 1, L \\right ] \\end{align} $$\n이 또한 트랜스포머와 유사한데, 인풋 시퀀스 \\(x = x_1 \\cdots x_{\\lvert x \\rvert}\\)를 (1) 시퀀스를 임베딩 벡터로 변환해주고, (2) 이전 블럭의 아웃풋을 이후 블럭의 인풋으로 사용하는 것의 반복입니다.\n들어가기에 앞서\n아래 나오는 \\( V_n, O_n \\) 등은 실제론 벡터로, 논문에서도 설명의 간편함을 위해 스칼라 사이의 mapping으로 설명하고 있습니다. 또한 논문에선 이를 구분하기 위해 소문자 표현을 사용했지만 저는 다음 규칙에 맞춰 작성하겠습니다.\nMatrix Vector Scalar \\( \\boldsymbol{A} \\) \\( \\vec {\\boldsymbol{A}} \\) \\( A \\) Retention Fig. 2. RetNet의 dual form.\nRetention mechanism의 가장 큰 특징은 두 가지 표현 방식을 가지는 것으로, 학습시엔 parallelism의 이점을, 추론시엔 RNN으로서의 이점을 활용합니다. 이를 위해 연구진은 어떤 recurrent state \\( \\vec {\\boldsymbol S}_n \\)을 사용하여 \\( V_n \\mapsto O_n \\)의 sequence modelling problem을 고려합니다. 당장 \\( V_n, O_n \\)등이 뜻하는 바가 뭔지 궁금하신 분들도 계시겠지만 이는 잠깐동안 미뤄놓겠습니다. 연구진은 recurrent state \\( \\vec {\\boldsymbol S}_n \\)과 \\( O_n \\)으로의 mapping을 다음과 같이 정의했습니다.\n$$ \\begin{align} \\vec {\\boldsymbol S_n} = \\boldsymbol{A} \\vec {\\boldsymbol S}_{n-1} + \\vec {\\boldsymbol K}_n V_n \\end{align} $$\n$$ \\begin{align} O_n = \\vec {\\boldsymbol Q}_n \\cdot \\vec {\\boldsymbol S}_n \\end{align} $$\n이때 \\( \\vec {\\boldsymbol Q}_n, \\vec {\\boldsymbol K}_n, V_n \\)는 아래와 같습니다. \\( V_n \\)이 스칼라라는 것만 제외하면 익숙한 형태입니다. 참고로 \u0026ldquo;들어가기에 앞서\u0026quot;에 설명했듯 이는 설명의 간편함을 위한 것으로, 실제 모델에선 \\( \\vec {\\boldsymbol V}_n \\) 또한 벡터고 \\({\\boldsymbol W}_V\\)도 매트릭스 입니다. $$ \\begin{align} \\vec {\\boldsymbol Q}_n \u0026amp;= \\vec {\\boldsymbol X}_n {\\boldsymbol W}_Q \\\\ \\vec {\\boldsymbol K}_n \u0026amp;= \\vec {\\boldsymbol X}_n {\\boldsymbol W}_K \\\\ V_n \u0026amp;= \\vec {\\boldsymbol X}_n \\cdot \\vec {\\boldsymbol W}_V \\end{align} $$\n다시 위로 돌아가서 식 (4)에 식 (3)을 대입해 \\( O_n \\)을 \\( \\vec {\\boldsymbol S}_i \\) 없이 표현할 수 있는지 보겠습니다.\n$$ \\begin{align} O_n \u0026amp;= \\vec {\\boldsymbol Q}_n \\cdot \\vec {\\boldsymbol S}_n \\\\ \u0026amp;= \\vec {\\boldsymbol Q}_n \\cdot \\left ( \\boldsymbol{A} \\vec {\\boldsymbol S}_{n-1} + \\vec {\\boldsymbol K}_n V_n \\right ) \\\\ \u0026amp;= \\vec {\\boldsymbol Q}_n \\cdot \\left ( \\boldsymbol{A} \\left ( \\boldsymbol{A} \\vec {\\boldsymbol S}_{n-2} + \\vec {\\boldsymbol K}_{n-1} V_{n-1} \\right ) + \\vec {\\boldsymbol K}_n V_n \\right ) \\\\ \u0026amp;= \\vec {\\boldsymbol Q}_n \\cdot \\left ( {\\boldsymbol A}^n \\vec {\\boldsymbol S}_0 + \\sum^n_{m=1} {\\boldsymbol A}^{n-m} \\vec {\\boldsymbol K}_m V_m \\right ) \\end{align} $$\n만약 \\( \\vec {\\boldsymbol S}_0 = \\vec 0 \\)이라면 식 (11)에서 state항을 없앨 수 있고, 논문에서 이를 명시적으로 서술하진 않지만 식 (12)에 해당하는 수식만 적혀있는걸 보면 \\( \\vec {\\boldsymbol S}_0 \\)는 \\( \\vec 0 \\)으로 초기화된다고 추측할 수 있습니다.\n$$ \\begin{align} O_n = \\vec {\\boldsymbol Q}_n \\cdot \\sum^n_{m=1} {\\boldsymbol A}^{n-m} \\vec {\\boldsymbol K}_m V_m \\end{align} $$\n여기서 더 나아가 \\( \\boldsymbol{A} \\)를 대각화 해주면 \\( \\boldsymbol{A} = \\boldsymbol{\\Lambda}\\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right ) \\boldsymbol{\\Lambda}^{-1} \\)로 표현할 수 있고, \\( \\boldsymbol{A}^{n-m} \\)은 다음과 같습니다.\n$$ \\begin{align} \\boldsymbol{A}^{n-m} = \\boldsymbol{\\Lambda}\\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{n-m} \\boldsymbol{\\Lambda}^{-1} \\end{align} $$\n이제 식 (13)을 식 (12)에 대입하여 정리해보겠습니다.\n$$ \\begin{align} O_n \u0026amp;= \\sum^n_{m=1} \\vec {\\boldsymbol Q}_n \\cdot \\boldsymbol{\\Lambda}\\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{n-m} \\boldsymbol{\\Lambda}^{-1} \\vec {\\boldsymbol K}_m V_m \\\\ \u0026amp;= \\sum^n_{m=1} \\vec {\\boldsymbol Q}^{\\prime}_n \\cdot \\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{n-m} \\vec {\\boldsymbol K}^{\\prime}_m V_m \\\\ \u0026amp;= \\sum^n_{m=1} \\left ( \\vec {\\boldsymbol Q}^{\\prime}_n \\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^n \\right ) \\cdot \\left ( \\left ( \\vec {\\boldsymbol{\\gamma}} e^{i \\vec {\\boldsymbol \\theta}} \\right )^{-m} \\vec {\\boldsymbol K}^{\\prime}_m \\right ) V_m \\end{align} $$\n\\( {\\boldsymbol W}_Q, {\\boldsymbol W}_K \\)가 학습 가능한 파라미터라는 점과 \\( \\boldsymbol{\\Lambda} \\)가 인풋으로부터 독립적인 값이라는 점을 고려해 프라임 표시를 떼고, \\( \\vec {\\boldsymbol \\gamma} \\)를 scalar로 reduce하면 \\( O_n \\)은 다음과 같아집니다.\n$$ \\begin{align} O_n = \\sum^n_{m=1} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\right ) \\cdot \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m \\end{align} $$\n\\( \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}}, \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\)는 xPos [2]의 형태로, 생긴대로 시퀀스 상에서의 위치 정보를 포함하고 있으며, relative position embedding의 일종으로 해석될 수 있습니다. 또한 식 (17)은 쉽게 parallelizable한 형태를 띄고 있으며, 이는 식 (4)의 recurrent한 형태와 함께 \\( O_n\\)이 두 가지 방식으로 계산될 수 있다는 것을 보여줍니다.\n이제 \\( O_n\\)을 attention score와 유사한 역할을 하는 어떤 값이라고 생각해보면, retention mechanism은 이 값을 학습시엔 트랜스포머처럼 병렬적으로 계산하지만 추론시엔 같은 파라미터를 이용함에도 recurrent하게 계산할 수 있어 quadratic한 코스트를 linear하게 줄일 수 있도록 하는 메커니즘이라고 할 수 있습니다.\nChunkwise Recurrent Representation 앞서 recurrent, parallel representation을 살펴봤는데 연구진은 여기서 그치지 않고 그 둘의 표현법을 섞어 활용하기도 했습니다. 우선 \\( \\vec{\\boldsymbol R}_n = \\sum^n_{m=1} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m \\)이라고 정의하면 식 (17)을 다음과 \\(b\\)의 길이만큼 잘라서 표현할 수 있습니다.\n$$ \\vec{\\boldsymbol R}_n = \\sum^n_{m=b} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m + \\gamma^b \\vec{\\boldsymbol R}_{n-b} $$\n즉, 각각의 chunk는 그대로 식 (17) 혹은 식 (19)의 inner-chunk와 같이 계산하고, chunk들 사이의 정보는 아래 식 (19)의 cross-chunk와 같이 계산할 수 있습니다. 이를 통해 연구진은 매우 긴 시퀀스도 일정한 길이의 chunk로 잘라 계산함으로써 학습 효율을 높일 수 있었다고 합니다.\n$$ \\begin{align} O_n \u0026amp;= \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\cdot \\vec{\\boldsymbol R}_n \\\\ \u0026amp;= \\underbrace{\\sum^n_{m=b} \\gamma^{n-m} \\left ( \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\right ) \\cdot \\left ( \\vec {\\boldsymbol K}_m e^{i m \\vec {\\boldsymbol \\theta}} \\right )^\\dagger V_m }_\\text{Inner-Chunk} + \\overbrace{\\gamma^b \\vec {\\boldsymbol Q}_n e^{i n \\vec {\\boldsymbol \\theta}} \\cdot \\vec{\\boldsymbol R}_{n-b} }^\\text{Cross-Chunk} \\end{align} $$\nGated Multi-Scale Retention Multi-Scale Retention (MSR) 또한 Multi-Head Attention (MHA)과 유사하게 \\(d_\\text{model}\\) 차원을 헤드의 차원 \\( d \\)로 나눠 여러 헤드를 사용했습니다. 각 헤드는 서로 다른 \\( {\\boldsymbol W}_Q, {\\boldsymbol W}_K, {\\boldsymbol W}_V \\in \\mathbb{R}^{d \\times d} \\) 파라미터를 가지며 각 헤드의 \\( \\gamma \\)는 다르지만, 레이어 별로는 동일합니다. 또한 비선형성을 증가시키기 위해 swish gate를 추가하였다고 합니다.\n$$ \\begin{align} \\gamma \u0026amp;= 1 - 2^{-5-\\text{arange}(0, h)} \\in \\mathbb R^h \\\\ \\text{head}_i \u0026amp;= \\text{Retention}(\\boldsymbol X, \\gamma_i ) \\\\ Y \u0026amp;= \\text{GroupNorm}_h (\\text{Concat}(\\text{head}_1, \\cdots, \\text{head}_h)) \\\\ MSR(\\boldsymbol X) \u0026amp;= (\\text{swish}({\\boldsymbol X} {\\boldsymbol W}_G) \\odot Y) {\\boldsymbol W}_O \\end{align} $$\n이때 \\( {\\boldsymbol W}_G, {\\boldsymbol W}_O \\in \\mathbb{R}^{d_\\text{model} \\times d_\\text{model}} \\) 역시 학습가능한 파라미터입니다.\n이외에도 Normalization 기법이나 pseudocode 등의 내용 또한 있으니 이런 보다 디테일한 사항이 궁금하신 분은 논문을 참고하시길 바랍니다.\nOverall Architecture of Retention Networks $$ \\begin{align} {\\boldsymbol Y}^l \u0026amp;= \\text{MSR}(\\text{LN}({\\boldsymbol X}^l)) + {\\boldsymbol X}^l \\\\ {\\boldsymbol X}^{l+1} \u0026amp;= \\text{FFN}(\\text{LN}({\\boldsymbol Y}^l)) + {\\boldsymbol Y}^l \\end{align} $$\n이때 \\( \\text{FFN}({\\boldsymbol X}) = \\text{gelu}({\\boldsymbol X}{\\boldsymbol W}_1) {\\boldsymbol W}_2 \\)의 일반적인 형태입니다.\nExperiments 대망의 성능을 볼 차례가 되었습니다. 디테일한 사항은 논문에 남겨두고, 간략하게 살펴보도록 하겠습니다.\nTable. 1. 크기와 학습 하이퍼 파라미터\nFig. 3. 모델 사이즈에 따른 perplexity 추이.\nTable. 2. 6.7B 모델의 Zero-Shot, 4-Shot 성능\nTable. 3. 크기별 학습 코스트. 추론 뿐만 아니라 학습시에도 트랜스포머 대비 효율적임을 볼 수 있습니다.\nFig. 4. 추론 코스트. Recurrent한 형태로 추론하기에 시퀀스 길이에 무관한 코스트를 보여줍니다.\nTable. 4. Ablation.\n마무리 무려 트랜스포머의 후계자라는 제목을 달고나온 논문으로, 몇 안되는 실험 결과로 확언할 순 없지만, 그 성능 격차가 트랜스포머의 첫 등장만큼 놀랍지는 않다고 생각합니다. 동시에 트랜스포머와 비슷하거나 조금 높은 성능을 보이는 동시에 학습과 추론시 코스트는 드라마틱하게 줄였으니 정말 논문의 내용처럼 뛰어난 점만 있다면, 시장이나 사회적 측면에서의 파급력은 상당할 수도 있지 않을까 하는 생각도 듭니다.\n다만 추론시 RNN처럼 state에만 의존하는 것은 아님에도, 어쨌든 retention score를 계산함에 있어 state에 인코딩된 context 정보에 의존한다는 점이 아주 긴 시퀀스에서의 성능을 궁금하게 만듭니다. 또한 autoregressive하게 생성된 문장의 퀄리티 또한 알고 싶지만 이런 예시가 제공되지 않아 다소 아쉽다고 느껴집니다.\n그럼에도 불구하고 dual form을 취한다는 발상과 그 결과물 자체는 놀랍습니다. 어떻게 보면 보다 transformer라는 명칭이 어울리는건 이쪽일지도 모르겠다는 생각을 하며 이만 마무리 하겠습니다.\n여담 현재 2023년 7월 20일 기준으로 다음주에 코드를 공개한다고 하니, 관심 있으신 분은 추후 링크를 참고하시길 바랍니다.\nReferences [1] Sun et al. \u0026ldquo;Retentive Network: A Successor to Transformer for Large Language Models\u0026rdquo; (2023).\n[2] Sun et al. \u0026ldquo;A Length-Extrapolatable Transformer\u0026rdquo; (2022).\n","permalink":"https://lexiconium.github.io/posts/retentive_network/","summary":"트랜스포머 아키텍쳐가 2017년에 등장한 이후 다양한 영역에서 그 유용성을 입증했고 따라서 현대 딥러닝 아키텍쳐의 표준으로 자리매김 했다 해도 과언이 아닐 것이라 생각합니다. 다만 그런 트랜스포머 아키텍쳐 (혹은 어텐션 메커니즘)에도 단점은 있으니, 바로 높은 코스트입니다. 특히 트랜스포머의 parallelism은 학습시엔 유용하지만 이를 추론 때 역시 강제하여 시퀀스 길이에 quadratic한 자원을 요구하게 합니다.\nFig. 1. \u0026ldquo;Impossible triangle\u0026rdquo;. RetNet(Retentive Network)은 불가능을 가능케 합니다. [1]\n이를 극복하기 위해 연구진은 두 가지 표현 방식(recurrent, parallel)을 취할 수 있는 retention mechanism을 제안합니다.","title":"Retentive Network: A Successor to Transformer for Large Language Models 리뷰"},{"content":"최근 수 년간 많은 pretrained 모델들이 다양한 downstream task들에서 활약하며 방대한 규모의, 논문에서 말하길 noisy internet-scale, dataset을 통한 pretraining 패러다임이 natural language processing 및 computer vision 분야에서 유효하다는 것이 증명됐다. 다만 이와 같은 방법론이 아직 널리 퍼지지 않은 분야가 있는데, 바로 로보틱스, 게이밍 및 컴퓨터 사용 등의 sequential decision 분야이다. 이에 대한 유인은 sequential decision과 관련된 data 그 자체는 풍부하나 그러한 data의 대부분이 직전 프레임에서 어떤 행동을 취해야 다음 프레임으로 넘어가는지에 대한 정보를 포함하고 있지 않음에 있다. 일례로 유튜브에는 수없이 많은 마인크래프트 플레이 영상이 있지만, 플레이어가 어떤 액션을 취하는지 직접적으로 레이블링이 되어 있는 경우는 없을 것이다. 이런 경우 reinforcement learning을 사용할 수도 있겠지만 이는 상대적으로 sample inefficiency를 유발하거나, 문제가 hard-exploration task인 경우 큰 비용이 요구될 수도 있다.\n이에 이들은 pretraining 패러다임을 sequential decision 분야로 확장함과 동시에 난해한 방법론을 피하기 위해 작은 규모의 labeled data로 큰 규모의 unlabeled data를 충분한 정확도로 labeling할 수 있는 inverse dynamics model (IDM)을 제시한다. 동시에 이렇게 labeling된 대규모 dataset을 behavioral cloning을 통해 학습시킨 모델, foundation model, 의 zero-shot 성능 그리고 현시점에선 reinforcement learning만으론 풀 수 없는 hard-exploration task를 foundation 모델을 imitation 및 reinforcement learning으로 fine-tuning함으로써 해결할 수 있음을 마인크래프트 환경을 통해 보인다.\nFig. 1. VPT method overview [1]\nInverse Dynamics Model (IDM) 상기했듯이 대다수의 data는 explicit action label을 포함하고 있지 않고, 따라서 현존하는 semi-supervised imitation learning 방법론들은 그러한 label 없이 학습하는 것을 목표로 하고 있다. 하지만 이 경우 환경을 탐색함에 있어 전적으로 policy에 의존하게 되고, 이는 exploration bottleneck이 발생할 시 policy가 절적한 행동을 학습할 수 없음을 의미한다.\n이들은 이런 한계점을 피하기 위해 거대한 unlabeled dataset을 pseudo labeling한 뒤 이를 통해 sequential decision 분야에 pretraining 방법론을 적용하고자 한다. 이들은 이들의 실험이 앞선 실험들에 비해 큰 규모로, 즉 많은 양의 data를 사용하여 진행되므로 텍스트와 같은 분야에서 이미 증명됐듯 간단한 방식임에도 좋은 성능을 이끌어낼 수 있을 것이라 가정한다. 또한 이 실험은 마인크래프트에 국한되지만, 보다 일반적으로 적용될 수 있는 방법론을 실험하기 위해 모델의 행동은 마우스 움직임과 키 입력으로 설정됐다. 이때 사용되는 data는 20 fps 영상이다.\n이때 이들은 pseudo labeling을 위한 모델을 학습시킴에 있어 behavioral cloning을 사용하는 대신 inverse dynamics modeling이라는 방법을 제시한다. 그 이유로 만일 Behavioral cloning을 통해 학습한다면 이 task는 과거의 관측을 통해 미래의 behavior를 추론하는 causal modeling task로, 이는 과거와 미래의 관측값을 모두 알고 있는 상태에서 그 사이에 어떤 행동을 취해야하는지 추론하는 inverse dynamics modeling task에 비해 학습 난이도가 높으며 따라서 많은 양의 data를 요구할 수도 있기 때문이라고 말한다.\nIDM은 대략 5 억 개의 학습가능한 가중치를 가진 모델로 128 개의 연속된 프레임을 입력으로 받으며 그 개략적인 구성은 다음과 같다. Temporal convolution layer, RestNet 기반의 image processing stack 그리고 키 입력과 마우스 움직임을 동시에 예측하는 residual unmasked attention layer. 저자들은 개중 첫 번째 구성요소인 temporal convolution layer의 중요성을 역설하는데, 이는 해당 layer가 시간축 정보를 통합해주는 역할을 한다는 점에서 이해할 수 있다. 물론 마지막 attention layer 또한 그 연산과정에 시간축을 활용하지만, 이를 단독으로 처리하는 동시에 temporal convolution layer를 포함했을 때와 비슷한 성능을 내려면 보다 크고 깊은 구성을 필요로하지 않을까 추측해본다.\nFig. 2. IDM이 temporal convolution layer를 포함했을 때와 포함하지 않았을 때의 loss 및 성능 비교 [2]\n마지막으로 pseudo labeling 과정을 살펴보면, 상기한 모델은 128 개의 연속된 프레임을 입력으로 받기 때문에 inference 단계에서도 당연히 128 개의 연속된 프레임을 입력 받는다. 다만 IDM의 진가는 non-causal한 objective로 학습했다는 점에 있고, 따라서 저자들은 시간축을 기준으로 sliding window 기법을 통해 예측값을 얻었으며 이때 stride로는 64 프레임을 사용한다. 또한 128 개의 프레임 중 32 번째부터 96 번째까지의 프레임에 대응되는 예측값만을 사용하여 영상의 처음과 마지막을 제외하곤 IDM 예측값의 경계 부분은 사용하지 않는다.\n그 외 IDM의 자세한 구성과 학습 방법 등은 논문의 Appendix D에서 볼 수 있다.\nPerformance Fig. 3. 좌측은 IDM의 키 입력 및 마우스 위치의 정확도를 contractor dataset size에 대해 나타냈고 우측은 IDM과 BC를 마찬가지로 dataset size에 대해 비교한 것이다. [2]\n저자들은 IDM을 1962 시간의 플레이 데이터에 학습시킨 결과 90.6%의 키 입력 정확도 그리고 마우스 움직임에서 0.97 \\(R^2\\)을 얻을 수 있었다. Fig. 3.의 좌측 그림에서 볼 수 있듯 IDM의 성능과 dataset의 크기는 여전히 양의 상관관계를 보이고 있지만, 1962 시간 이상으로 크기를 키우는 것은 효율이 떨어진다고 판단한 듯 보인다.\n또한 BC와 IDM의 성능 비교를 통해 그들의 가정이 옳았음을 보였고, 같은 성능일 때 적어도 이 환경과 모델에선 BC가 대략 30 배 가까운 data를 필요로 한다는 것을 Fig. 3.의 우측 그림에서 확인할 수 있다.\nData Filtering 이들은 인터넷에서 관련 키워드로 수집한 영상을 사용하기 전 플레이어의 얼굴, 채널 로고, 워터마크 등의 시각적 결함이나 PC 외의 플랫폼에서 플레이 된 영상 그리고 survival mode 외의 모드로 플레이된 영상 등을 제거하기 위해 data filtering 과정을 수행했다. 다만 충분히 많은 data, 충분히 큰 모델 그리고 충분한 연산능력이 뒷받침 된다면 해당 BC 모델 또한 이런 filtering 과정 없이도 깨끗한 마인크래프트 환경에서 잘 작동하리라 생각하지만, 간단함과 효율성을 위해 filtering 과정을 수행한다고 덧붙였다. 이 과정은 인터넷에서 수집한 영상에서 샘플링된 8800 개의 프레임을 학습한 모델이 수행한다.\n어떤 키워드로 영상을 수집했는지 그리고 filtering 모델에 대한 보다 자세한 정보 등은 논문의 Appendix A에서 볼 수 있다.\nVideo PreTraining (VPT) VPT Foundation Model Training and Zero-Shot Performance 수집된 27 만 시간 정도의 영상 중 위의 filtering 과정을 거쳐 7 만 시간 정도의 영상이 학습에 사용됐다. 또한 앞선 연구에 의해 제시된 5 억 파라미터, 30 epoch을 학습시켰으며 이는 720 개의 V100 GPU를 동원했을 때 9 일이 걸렸다고 한다. 이때 VPT의 training objective는 과거 관측값이 주어졌을 때 IDM에 의해 예측된 action에 대한 negative log-likelihood를 최소화하는 것으로 다음과 같이 쓸 수 있다. [2]\n$$ \\min_{\\theta} \\sum_{t \\in \\left [ 1\u0026hellip;T \\right ]} -\\log \\pi_{\\theta}\\left( a_t | o_1, \u0026hellip;, o_t \\right), a_t \\text{\\textasciitilde} p_{\\text{IDM}}\\left( a_t | o_1, \u0026hellip;, o_t \\right) $$\nFig. 4. 좌측은 IDM에 의해 pseudo labeling된 dataset에 대한 train 및 valid loss 그리고 GT인 contractor data에 대한 loss를 training epoch에 따라 나타낸 것이다. 우측은 각 아이템이 에피소드 당 몇 개 수집됐는지 2500 개 이상의 60 분 survival 에피소드를 평균내서 training epoch에 따라 나타낸 것이다. [2]\n한 가지 특기할만한 점은 Fig. 4.의 좌측 그림에서 볼 수 있듯이 train, valid loss는 학습에 따라 꾸준히 감소하는 반면에 contractor data에 대한 loss는 7 epoch에서 최저값을 달성하고 반등한다는 점이다. 이는 그리 직관적인 현상은 아니라 저자들은 이에 대해 Appendix H에서 다음과 같이 실험 결과를 통해 설명한다.\nFig. 5. Dataset과 모델 크기에 따른 loss 그리고 zero-shot 성능 [2]\n우선 이들은 Fig.5.가 나타내듯 여러 크기의 모델을 실험해봤다. 재밌게도 가장 작은 모델이 가장 낮은 contractor dataset loss를 보임과 동시에 가장 높은 web clean dataset loss를 보인다. 유사하게 zero-shot 성능 또한 가장 작은 모델이 가장 높은 것을 볼 수 있다.\nFig. 6. Foundation 모델을 contractor_house dataset에 fine-tune함에 따른 loss 변화 [2]\n하지만 해당 모델을 contractor_house라는 자체적인 게임 엔진에서 나온 dataset으로 fine-tuning하는 경우 이 순서가 단 몇 training step만에 바로 역전되는 것을 Fig. 6.에서 볼 수 있다. 저자들은 우선 가장 큰 모델이 contractor_house dataset에 대한 fine-tuning 이전에 높은 loss와 낮은 zero-shot 성능을 보이는 것은 web data의 특정 시각적 특이사항에 과도하게 집중함에 따라 나타나는 현상일 수 있다고 말한다. 동시에 단 몇 training step만에 그 순서가 역전되는 것은 그만큼 큰 모델이 data에 따라 더 나은 성능을 위해 빠르게 low level feature를 변환할 수 있기 때문이라고 말한다.\n이를 필자 나름대로 해석을 덧붙여가며 이해해보자면 우선 모델의 크기가 클수록 보다 그만큼 성능이 좋으니, 혹은 비유하자면 해상도가 높으니, 미세한 특성에 집중하게될 가능성이 생기고 그에 따라 성능 하락을 불러올 수 있다는 것이다. 또한 그만큼 많은 특성을 다루고 있으니 새로운 data에 발맞춰 빠르게 low level feature를 변환할 수 있다는 것.\n다만 이는 어디까지나 가설이고 완전히 검증된 것은 아니라고, not conclusive, 저자들도 말미에 덧붙이니 이 설명에 납득하거나 하지 않는 것은 결국 독자의 몫이라 할 수 있다.\nVPT 학습에 대한 보다 자세한 정보는 논문의 Appendix H에서 확인할 수 있다.\nFine-Tuning Performance Behavioral Cloning (BC) Fig. 7. 좌측은 foundation 모델의 zero-shot 성능 그리고 이를 earlygame_keyword 및 contractor_house dataset에 BC fine-tuning한 성능을 나타낸다. fine-tuning 후 foundation 모델은 하지 못했던 wooden, stone tools 제작이 가능해진 것을 볼 수 있다. 우측은 BC fine-tuning에 foundation 모델의 training 정도가 미치는 영향을 epoch에 대해 나타낸 것이다. [2]\nFoundation 모델을 contractor_house dataset에 BC fine-tuning한 뒤 wooden tools 및 stone tools 제작이 가능해졌는데 이는 숙련된 마인크래프트 플레이어가 제작할 시 평균 1.2 분 (1390 action) 그리고 2.3 분 (2790 action)이 걸린다고 한다. 여기서 contractor_house dataset이란 contractor에게 약 10 분을 주고 그 안에 기초적인 집을 짓게 한 dataset인데, 이를 위해선 wood, sand 그리고 dirt가 필요하다고 한다. 따라서 우리는 왜 earlygame_keyword dataset은 위와 같은 새로운 특성을 가르치지 못했지만 contractor_house dataset은 가르칠 수 있었는지를 유추해 볼 수 있다.\n한 가지 짚고 넘어갈만한 사항은 Fig. 4.의 우측 그림에서 볼 수 있듯 foundation 모델의 few-shot 성능은 training epoch에 따라 증가하지 않고 saturate되는 양상을 보이는데 반해 foundation 모델을 BC fine-tuning하는 경우 foundation 모델의 training epoch이 증가함에 따라 그 성능이 따라 증가한다는 것이다.\nFig. 8. IDM 품질의 영향은 IDM trainig data의 크기가 100 시간을 넘긴 이후론 크지 않다는 것을 볼 수 있다. [2]\nReinforcement Learning (RL) RL fine-tuning의 경우 학습에 앞서 reward shaping이 수행됐다. 자세한 정보는 논문의 Appendix G에서 볼 수 있다.\nFig. 9. (a) Episode에 따른 reward. 모델이 random initialize된 경우 그 어떤 reward도 얻지 못함을 볼 수 있다. (b) 모델이 random initialize된 경우 그 어떤 요소도 수집하거나 제작하지 못한다. (c) Foundation 모델을 RL fine-tuning한 경우. (d) Foundation 모델을 위의 earlygame_keyword에 fine-tuning한 뒤 RL fine-tuning한 경우 (c)에서 습득하거나 제작하지 못했던 나머지 것들을 수집 및 제작할 수 있게 됐다. [2]\nFig. 10. Diamond pickaxe 제작 과정 [1]\nFoundation 모델 \\(\\rightarrow\\) Early-Game 모델 \\(\\rightarrow\\) RL fine-tuned 순으로 학습된 모델의 경우 놀라운 성능을 보여준다. 해당 모델은 iron pickaxe를 제작할 확률이 80%, diamond를 수집할 확률이 20% 그리고 diamond pickaxe를 제작할 확률이 2.5% 가량 되는데, 이는 사람이 diamond pickaxe를 제작하는 것을 목표로 하고 플레이 했을 때 각 아이템을 수집, 제작할 확률 57%, 15%, 12%와 비견될만한 수치이다.\nData Scaling Properties Fig. 11. Foundation 모델의 zero-shot 성능은 어느순간 정체되는 것을 볼 수 있는 반면 fine-tuning하는 경우 training data의 크기가 증가함에 따라 그 성능 또한 향상되는 것을 볼 수 있다. [2]\nConclusion 이들은 이번 실험을 통해 풍부한 unlabeled data를 sequential decision 분야에서 어떻게 사용하는 좋을지에 대해 하나의 방법을 제시했다고 자평한다. 또한 generative video modeling 혹은 contrastive method들이 representational prior만을 만들어내는데 반해 VPT는 어떻게 행동하면 되는지를 pretraining 단계에서 직접적으로 학습할 수 있게 하고 이는 이후 RL fine-tuning 단계에서 매우 효과적인 exploration prior를 제공한다고 한다.\n다만 현재 단계에서는 모델이 단지 과거의 관측값에 따라 행동할 뿐, 어떤 목표를 가지고 행동할지 설정할 순 없다. 이를 더 많은 data, 더 큰 모델로 학습하는 것과 더불어 향후의 과제로 남겨뒀다.\n여담 매우 생소한 방법론을 제시했다거나 한 연구는 아니었지만, 그 뛰어난 인력과 자본력을 동원해 일반적으로 확인하기 어려운 영역을 탐구하고, 놀랄만한 결과를 냈다는 점에서 역시 OpenAI다 하며 읽었다. 다만 읽는 내내 이들이 conclusion에서 말했듯 모델에 목표를 설정할 수 있다면 좋을텐데라는 생각을 했는데, 향후 연구에서 어떻게 목표를 설정할지, 그리고 설정했을 시 어느정도 정확도로 이를 달성할 수 있을지 등 향방이 기다려지는 논문이었다.\n참고로 이 연구에 쓰인 모델 weight 등은 공개되어 있으니 관심있는 독자는 VPT GitHub를 참고 바란다.\nReferences [1] https://openai.com/blog/vpt\n[2] Baker et al. \u0026ldquo;Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\u0026rdquo; (2022).\n","permalink":"https://lexiconium.github.io/posts/openai_vpt/","summary":"최근 수 년간 많은 pretrained 모델들이 다양한 downstream task들에서 활약하며 방대한 규모의, 논문에서 말하길 noisy internet-scale, dataset을 통한 pretraining 패러다임이 natural language processing 및 computer vision 분야에서 유효하다는 것이 증명됐다. 다만 이와 같은 방법론이 아직 널리 퍼지지 않은 분야가 있는데, 바로 로보틱스, 게이밍 및 컴퓨터 사용 등의 sequential decision 분야이다. 이에 대한 유인은 sequential decision과 관련된 data 그 자체는 풍부하나 그러한 data의 대부분이 직전 프레임에서 어떤 행동을 취해야 다음 프레임으로 넘어가는지에 대한 정보를 포함하고 있지 않음에 있다.","title":"OpenAI의 Video PreTraining (VPT) 리뷰"}]